{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch \n",
    "import torch.nn as nn "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv = nn.Conv1d(1, 1, 3, bias=False)\n",
    "conv.weight.data = torch.tensor(np.array([1., 2., 3.]), dtype=torch.float32).view(conv.weight.shape)\n",
    "\n",
    "x = torch.tensor(np.ones((1, 1, 10)), dtype=torch.float32, requires_grad=True)\n",
    "y = conv(x)\n",
    "y.sum().backward()\n",
    "#x.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1  3  6  9 12 15 18 21 24 17]\n",
      "[ 1  3  6  9 12 15 18 21 24 17]\n"
     ]
    }
   ],
   "source": [
    "# 1D\n",
    "\n",
    "N = 10\n",
    "inp = np.arange(N)\n",
    "pad = 1\n",
    "x = np.pad(inp, (pad, pad))\n",
    "\n",
    "n = x.shape[0]\n",
    "k = 3\n",
    "nout = n - k + 1\n",
    "s = x.strides[0]\n",
    "\n",
    "u = np.lib.stride_tricks.as_strided(x, (nout, k), (s, s))\n",
    "w = np.array([1, 1, 1])\n",
    "\n",
    "y = u @ w.reshape(-1, 1).squeeze()\n",
    "\n",
    "print(y)\n",
    "print(np.convolve(x, w, mode='valid'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "out:  28 28\n",
      "(64, 784, 3)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(64, 3, 28, 28)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "same:  True\n",
      "same set:  True\n"
     ]
    }
   ],
   "source": [
    "from numpy.lib.stride_tricks import as_strided, sliding_window_view # <3\n",
    "from collections import Counter\n",
    "\n",
    "c = 3           # in channels\n",
    "b = 64          # batches\n",
    "h, w = 28, 28   # height, width (for 1D set h=channels, w=length)\n",
    "ph, pw = 1, 1   # padding       (for 1D set ph=0, pw=padding)\n",
    "cout = 3        # out channels\n",
    "kh, kw = 3, 3   # filter size   (for 1D set kh=channels)\n",
    "\n",
    "# only needed for debugging purposes\n",
    "wout = w + 2*pw - kw + 1\n",
    "hout = h + 2*ph - kh + 1\n",
    "print(\"out: \", hout, wout)\n",
    "\n",
    "inp = np.arange(b*c*h*w).reshape(b, c, h, w)\n",
    "inp = np.random.randn(b, c, h, w)\n",
    "x = np.pad(inp, ((0, 0), (0, 0), (ph, ph), (pw, pw)))\n",
    "# display(x)\n",
    "\n",
    "u = sliding_window_view(x, (b, c, kh, kw)) # get all subarrays (essentially free operation, only uses stride tricks)\n",
    "assert u.shape == (1, 1, hout, wout) + (b, c, kh, kw) # can index into each subarray nicely, but we just want to flatten everything\n",
    "u = u.reshape(-1, b, c*kh*kw) # flatten each subarray, except along batch dim\n",
    "u = u.swapaxes(0, 1)      # put batch dim as first dim, for matmul later. 2nd dim contains each subarray \n",
    "assert u.shape == (b, hout*wout, c*kh*kw)\n",
    "\n",
    "filtr = np.ones((cout, c, kh, kw)) # filter, given as input to function\n",
    "filtr = np.random.randn(cout, c, kh, kw)\n",
    "\n",
    "w = filtr.reshape((cout, c*kh*kw)).T\n",
    "assert w.shape == (c*kh*kw, cout)\n",
    "\n",
    "y = u @ w  # do the actual computation (!!)\n",
    "print(y.shape)\n",
    "\n",
    "out = y.swapaxes(2, 1).reshape(b, cout, hout, wout) # unravel image and put channel dim before hout,wout dims\n",
    "display(out.shape)\n",
    "\n",
    "conv = nn.Conv2d(c, cout, (kw, kh), stride=1, padding=(ph, pw), bias=False)\n",
    "conv.weight.data = torch.tensor(filtr, dtype=torch.float32, requires_grad=True)\n",
    "x = torch.tensor(inp, dtype=torch.float32, requires_grad=True)\n",
    "y = conv(x)\n",
    "\n",
    "yt = y.data.numpy()\n",
    "ym = out.astype(np.float32)\n",
    "yts = np.array(list(sorted(set(yt.flatten().round(2)))))\n",
    "yms = np.array(list(sorted(set(ym.flatten().round(2)))))\n",
    "\n",
    "print(\"same: \", np.allclose(yt, ym, atol=1e-4))\n",
    "print(\"same set: \",  np.allclose(yts, yms, atol=1e-4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 3, 28, 28) (10, 3, 28, 28)\n",
      "1 1\n",
      "(10, 3, 28, 28) (3, 3, 3, 3)\n",
      "(10, 3, 28, 28)\n",
      "(3, 10, 30, 30) (3, 10, 28, 28)\n"
     ]
    }
   ],
   "source": [
    "from numpy.lib.stride_tricks import sliding_window_view\n",
    "\n",
    "def pad2D(x, ph, pw):\n",
    "    \"\"\"Pads if p > 0, crops if p < 0. `x` is assumed to have shape (b, c, h, w).\n",
    "\n",
    "    Both padding and cropping is symmetric, so output will have shape (b, c, h+2*ph, w+2*pw)\"\"\"\n",
    "    assert x.ndim == 4 \n",
    "    if ph < 0: x = x[:, :, -ph:ph, :]\n",
    "    if pw < 0: x = x[:, :, :, -pw:pw]\n",
    "    return np.pad(x, ((0, 0), (0, 0), (max(0,ph), max(0,ph)), (max(0,pw), max(0,pw))))\n",
    "\n",
    "# like 10x slower than torch.nn.Conv2D :(\n",
    "def convolve2D(inp: np.ndarray, filtr: np.ndarray, padding: tuple[int,int] = (0, 0)) -> np.ndarray:\n",
    "    assert inp.ndim == filtr.ndim == 4, (inp.ndim, filtr.ndim)\n",
    "    (b, c, h, w) = inp.shape \n",
    "    (cout, cprime, kh, kw) = filtr.shape \n",
    "    assert cprime == c, (inp.shape, filtr.shape)\n",
    "    \n",
    "    ph, pw = padding\n",
    "    x = pad2D(inp, ph, pw)\n",
    "\n",
    "    # only needed for debugging purposes\n",
    "    wout = w + 2*pw - kw + 1\n",
    "    hout = h + 2*ph - kh + 1\n",
    "    # print(\"out: \", hout, wout)\n",
    "\n",
    "    u = sliding_window_view(x, (b, c, kh, kw)) # get all subarrays (essentially free operation, only uses stride tricks)\n",
    "    assert u.shape == (1, 1, hout, wout) + (b, c, kh, kw)\n",
    "    u = u.reshape(hout*wout, b, c*kh*kw) # flatten each subarray, except along batch dim (TODO: this is slow for some reason)\n",
    "    u = u.swapaxes(0, 1) # put batch dim as first dim, for matmul later. 2nd dim contains each subarray \n",
    "    assert u.shape == (b, hout*wout, c*kh*kw)\n",
    "\n",
    "    w = filtr.reshape((cout, c*kh*kw)).T\n",
    "    assert w.shape == (c*kh*kw, cout)\n",
    "\n",
    "    y = u @ w # do the actual computation\n",
    "\n",
    "    assert y.shape == (b, hout*wout, cout)\n",
    "    out = y.swapaxes(1, 2).reshape(b, cout, hout, wout)\n",
    "    return out\n",
    "\n",
    "c = 3           # in channels\n",
    "b = 10          # batches\n",
    "h, w = 28, 28   # height, width (for 1D set h=channels, w=length)\n",
    "ph, pw = 1,1   # padding       (for 1D set ph=0, pw=padding)\n",
    "cout = 3        # out channels\n",
    "kh, kw = 3, 3   # filter size   (for 1D set kh=channels)\n",
    "\n",
    "inp = np.random.randn(b, c, h, w)\n",
    "filtr = np.random.randn(cout, c, kh, kw)\n",
    "out = convolve2D(inp, filtr, (ph, pw))\n",
    "\n",
    "print(inp.shape, out.shape)\n",
    "\n",
    "# simulate gradient \n",
    "gy = np.ones_like(out) \n",
    "\n",
    "# calculate padding \n",
    "pyh = kh - ph - 1\n",
    "pyw = kw - pw - 1\n",
    "print(pyh, pyw)\n",
    "\n",
    "filtr_rev = np.flip(filtr, (-1, -2)).swapaxes(0, 1)\n",
    "print(gy.shape, filtr_rev.shape)\n",
    "gx = convolve2D(gy, filtr_rev, (pyh, pyw))\n",
    "print(gx.shape)\n",
    "\n",
    "assert gx.shape == inp.shape\n",
    "\n",
    "x = pad2D(inp, ph, pw).swapaxes(0,1)\n",
    "y = gy.swapaxes(0,1)\n",
    "print(x.shape, y.shape)\n",
    "\n",
    "gw = convolve2D(pad2D(inp, ph, pw).swapaxes(0,1), gy.swapaxes(0,1), (0,0)).swapaxes(0,1)\n",
    "\n",
    "# gw = convolve2D(x, y, (0, 0)).swapaxes(0,1)\n",
    "assert gw.shape == filtr.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "same fwd: True 3.8491405724983e-06\n",
      "same bwd gx: True 7.105834693987845e-07\n",
      "same bwd gw: True 7.957397089342066e-05\n"
     ]
    }
   ],
   "source": [
    "# pytorch equiv\n",
    "conv = nn.Conv2d(c, cout, (kw, kh), stride=1, padding=(ph, pw), bias=False)\n",
    "conv.weight.data = torch.tensor(filtr, dtype=torch.float32, requires_grad=True)\n",
    "x = torch.tensor(inp, dtype=torch.float32, requires_grad=True)\n",
    "y = conv(x)\n",
    "yt = y.data.numpy()\n",
    "ym = out \n",
    "\n",
    "print(\"same fwd:\", np.allclose(yt, ym, atol=1e-4), (yt - ym).max())\n",
    "\n",
    "y.sum().backward()\n",
    "gxt = x.grad.data.numpy() \n",
    "gxm = gx\n",
    "gwt = conv.weight.grad.data.numpy()\n",
    "gwm = gw\n",
    "\n",
    "print(\"same bwd gx:\", np.allclose(gxt, gxm, atol=1e-4), np.abs(gxt - gxm).max())\n",
    "print(\"same bwd gw:\", np.allclose(gwt, gwm, atol=1e-2), np.abs(gwt - gwm).max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from micrograd.nn import Conv2D \n",
    "from micrograd.core import Tensor\n",
    "from micrograd.conv import convolve2D, pad2D\n",
    "\n",
    "c = 3           # in channels\n",
    "b = 10          # batches\n",
    "h, w = 28, 28   # height, width (for 1D set h=channels, w=length)\n",
    "ph, pw = 1,1   # padding       (for 1D set ph=0, pw=padding)\n",
    "cout = 3        # out channels\n",
    "kh, kw = 3, 3   # filter size   (for 1D set kh=channels)\n",
    "\n",
    "x = Tensor(np.random.randn(b, c, h, w))\n",
    "conv = Conv2D(c, cout, (kh, kw), (ph, pw))\n",
    "y = conv(x)\n",
    "y.sum().backward()\n",
    "\n",
    "xt = torch.tensor(x.data, requires_grad=True)\n",
    "convt = nn.Conv2d(c, cout, (kh, kw), 1, (ph, pw), bias=False)\n",
    "convt.weight.data = torch.tensor(conv.w.data)\n",
    "yt = convt(xt)\n",
    "yt.sum().backward()\n",
    "\n",
    "assert np.allclose(y.data, yt.data.numpy(), atol=1e-5)\n",
    "assert np.allclose(x.grad.data, xt.grad.data.numpy(), atol=1e-4)\n",
    "assert np.allclose(conv.w.grad.data, convt.weight.grad.data.numpy(), atol=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[[40,  9, 18,  5, 48],\n",
       "         [10, 28, 13, 22,  7],\n",
       "         [19, 25, 47, 20, 26],\n",
       "         [ 4, 29, 38, 30, 16],\n",
       "         [15,  1,  6, 34, 33]],\n",
       "\n",
       "        [[39, 12, 11, 35, 31],\n",
       "         [24, 43, 46, 21, 27],\n",
       "         [45, 23,  2, 37,  8],\n",
       "         [ 0, 17, 32, 49, 42],\n",
       "         [44, 36, 41, 14,  3]]]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# max pooling:\n",
    "\n",
    "c = 2           # in channels\n",
    "b = 1           # batches\n",
    "h, w = 5, 5 # height, width \n",
    "ph, pw = 0, 0   # padding     \n",
    "kh, kw = 3, 3   # filter size \n",
    "\n",
    "def pool2d(x, kersize: tuple[int,int], ):\n",
    "    b, c, h, w = x.shape \n",
    "    kh, kw = kersize\n",
    "    hout = h + 2*ph - kh + 1\n",
    "    wout = w + 2*pw - kw + 1\n",
    "    # print(hout, wout)\n",
    "\n",
    "    u = sliding_window_view(x, (b, c, kh, kw)).squeeze((0,1))\n",
    "\n",
    "    y = u.max((-1, -2)).transpose(2,3,0,1)\n",
    "\n",
    "    uflat = u.reshape(hout, wout, b, c, -1)\n",
    "\n",
    "    idx = np.argmax(uflat, -1).transpose(2, 3, 0, 1)\n",
    "    ys, xs = np.unravel_index(idx, (kh, kw))\n",
    "    ys = ys + np.arange(hout).reshape(-1,1)\n",
    "    xs = xs + np.arange(wout).reshape(1,-1)\n",
    "\n",
    "    linidx = (w*ys + xs).reshape(b, c, -1)\n",
    "\n",
    "    gx = np.zeros((b, c, w*h))\n",
    "    gy = np.ones_like(y).reshape(b, c, -1)\n",
    "\n",
    "    it = np.nditer(linidx, flags=['multi_index'])\n",
    "    for idx in it:\n",
    "        i, j, k, = it.multi_index\n",
    "        gx[i, j, idx] += gy[i,j,k]\n",
    "\n",
    "    gx = gx.reshape(b,c,h,w)\n",
    "\n",
    "    return y, gx\n",
    "\n",
    "\n",
    "x = np.arange(b*c*h*w).reshape(b,c,h,w)\n",
    "x = x.ravel(); np.random.shuffle(x); x = x.reshape(b, c, h, w)\n",
    "display(x)\n",
    "\n",
    "y, gx = pool2d(x, (kh, kw))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [],
   "source": [
    "pool = nn.MaxPool2d((kh, kw), 1)\n",
    "xt = torch.tensor(x, dtype=torch.float32, requires_grad=True)\n",
    "yt = pool(xt)\n",
    "yt.sum().backward(retain_graph=True)\n",
    "\n",
    "xt.grad\n",
    "assert np.allclose(yt.data.numpy(), y)\n",
    "assert np.allclose(xt.grad.data.numpy(), gx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([1, 2, 3]), array([0, 2, 3]))"
      ]
     },
     "execution_count": 299,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = np.array([[1,1,2,3,1,3,2]])\n",
    "np.unique(a, axis=None, return_index=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
